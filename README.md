#  ETL Orders Pipeline

   ##  DescripciÃ³n general

   Este proyecto implementa una **ETL batch** que consume datos de Ã³rdenes desde una **API REST mock**, los transforma y los prepara en un **modelo analÃ­tico dimensional** listo para anÃ¡lisis.

   La soluciÃ³n estÃ¡ diseÃ±ada para ser:
   - Ejecutable localmente
   - Modular y mantenible
   - Testeable
   - Alineada a buenas prÃ¡cticas de ingenierÃ­a de datos

   Cumple con todos los entregables obligatorios definidos en la prueba tÃ©cnica.

---
```text
## ğŸ—ï¸ Arquitectura de la soluciÃ³n
      
   API Mock (REST - mockapi.io)
         â†“
   ETL Batch (Python)
         â†“
   output/raw        â†’ JSON original
   output/curated    â†’ Datos curados (CSV / Parquet)
         â†“
   Modelo dimensional (DDL SQL)

```

---

## ğŸ“ Estructura del proyecto

```text
etl-test/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ api_orders.json     # Ejemplo de payload de la API
â”‚   â”œâ”€â”€ users.csv           # Datos de referencia (dim_user)
â”‚   â””â”€â”€ products.csv        # Datos de referencia (dim_product)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl_job.py          # Orquestador principal de la ETL
â”‚   â”œâ”€â”€ api_client.py       # Cliente para consumo de la API REST
â”‚   â”œâ”€â”€ transforms.py       # LÃ³gica de transformaciÃ³n y normalizaciÃ³n
â”‚   â””â”€â”€ db.py               # AbstracciÃ³n de carga / persistencia
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ redshift-ddl.sql    # DDL compatible con Redshift / DuckDB
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transforms.py  # Tests unitarios (pytest)
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ raw/                # Datos originales desde la API
â”‚   â””â”€â”€ curated/            # Datos transformados para analÃ­tica
â”‚
â””â”€â”€ docs/
    â””â”€â”€ design_notes.md     # Decisiones de diseÃ±o
```
---

## CÃ³mo ejecutar la soluciÃ³n

 ###  Requisitos

   - Python 3.9 o superior
   - pip

   Instalar dependencias:

   ```bash
   pip install -r requirements.txt
   ```

   ## 2 Fuente de datos â€“ API Mock

   La fuente de datos utilizada en este proyecto es una **API REST mock**, diseÃ±ada para simular un sistema transaccional de Ã³rdenes de compra.

   La API expone un endpoint que retorna un arreglo de Ã³rdenes en formato JSON, con estructuras anidadas que incluyen los Ã­tems del pedido y metadatos asociados.

   ### Endpoint

   https://695fb97f7f037703a814a280.mockapi.io/api/v1/orders

   ### Ajustes sobre los datos generados por la API Mock

   La API mock generÃ³ inicialmente datos con una estructura genÃ©rica (campos vacÃ­os, tipos inconsistentes y timestamps en formato Unix).  
   Posteriormente, estos datos fueron ajustados para alinearlos con el modelo esperado por la prueba tÃ©cnica y permitir una correcta transformaciÃ³n analÃ­tica.

   Los ajustes incluyeron normalizaciÃ³n de tipos, enriquecimiento del detalle de Ã­tems y estandarizaciÃ³n de formatos de fecha.
   ---
   ## 3 EjecuciÃ³n del proceso ETL

      Desde la raÃ­z del proyecto ejecutar:

      ```bash
      python src/etl_job.py
      

## El proceso ETL ejecuta las siguientes etapas:

   1. Consumo batch de Ã³rdenes desde la API REST mock
   2. Almacenamiento del JSON original en la capa `output/raw`
   3. TransformaciÃ³n y normalizaciÃ³n de los datos (explosiÃ³n del array `items`)
   4. GeneraciÃ³n de datasets curados listos para anÃ¡lisis en `output/curated`

   ## Resultados generados

   ### output/raw

   Contiene los datos originales tal como son entregados por la API REST, sin aplicar transformaciones.
   Esta capa permite trazabilidad y auditorÃ­a del proceso ETL.

   ### output/curated

   Contiene los datos transformados y normalizados.
   El nivel de granularidad es **item de pedido**, permitiendo anÃ¡lisis por producto, usuario y tiempo.

##  Tests

Los tests estÃ¡n implementados utilizando **pytest** y cubren:

- Transformaciones de datos
- NormalizaciÃ³n de estructuras anidadas (`items`)
- Validaciones bÃ¡sicas de consistencia

Para ejecutar los tests:
   ```bash
   pytest tests/ 

```

---

##  Modelo de datos

   El archivo `sql/redshift-ddl.sql` contiene el DDL del modelo analÃ­tico dimensional,
   compatible con **Amazon Redshift y DuckDB**.

   El modelo estÃ¡ compuesto por:

   - `dim_user`
   - `dim_product`
   - `fact_order` (tabla de hechos a nivel de item)

   Se utiliza un **Star Schema**, optimizado para consultas analÃ­ticas.

